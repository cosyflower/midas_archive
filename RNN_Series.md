
-> 은닉층에서 나온 결과값이 다시 은닉층으로 돌아가 새로운 입력값을 형성하고 연산을 수행하는 순환구조를 말한다. 완전 연결층, CNN은 은닉층에서 나온 결과값이 출력층 방향으로 이동하지만 RNN은 **은닉층**으로 되돌아가 순환한다는 점에서 차이가 있다!!

![[Pasted image 20240814094810.png]]


#### RNN - Vanishing Gradient problem 

LSTM and GRU - Gate mechanism에 대해서 
이전의 상태를 어떻게 반영할 것이고 현재 상태와 이전 상태의 업데이트 반영을 어느 수준으로 진행할지를 명시한 알고리즘 

**nn.Rnn, nn.GRU - 이미 function 형태로 지원되는 것도 인지하고 있으면 되겠다


### LSTM and GRU

- GRU : Update, Reset gate 두 개의 게이트가 존재한다
	- 각각에 대해서 learnable한 matrix를 배정한다 (reset, update 각각의 matrix를 구성한 상황)
	- 각각 sigmoid함수를 통과시킴으로써  0 - 1 로 다시 변환을 해준다
	- 각각에 대해서 의미 부여를 한 것인지??? 


![[Pasted image 20240814133622.png]]

![[Pasted image 20240814133631.png]]

![[Pasted image 20240814133639.png]]


새로운 은닉 상태를 만드는 경우 reset gate를 반영하게 된다(이전까지의 정보를 반영할 지 말지를 결정한다고 생각하면 되겠다) -> 이후에 최종적으로 은닉 상태를 계산하는 과정에서 업데이트 반영 정도를 활용해서 이전의 상태 그리고 현재의 상태를 반영하는 것으로 생각하면 되겠다. 

-> 정리하면 이전의 정보를 반영하는 정도를 확인
-> 정도까지 반영된 이전의 정보를 어느정도 업데이트 시킬 것인지를 결정한다고 생각하면 되겠다